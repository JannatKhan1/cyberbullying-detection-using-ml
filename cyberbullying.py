# -*- coding: utf-8 -*-
"""Cyberbullying.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dLBHj0NHaraqYh4R1dnnjR7zlgFKmSc4
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
import nltk
nltk.download('omw-1.4')
import nltk
nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
import nltk
nltk.download('words')
from nltk.corpus import words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.metrics import plot_confusion_matrix, classification_report
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV
import lightgbm as lgb



from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Indian Cyberbullying Datasets.csv")

df

df['Label'].unique()

df.info()

print(df.duplicated().sum())

df.head()

from gensim.parsing.preprocessing import remove_stopwords

def stopword_removal(row):
  text = row['Script']
  text = remove_stopwords(text)
  return text

df['Script']=df.apply(stopword_removal, axis=1)

df.head()

df['Script'] = df['Script'].str.lower().str.replace('[^\w\s]',' ').str.replace('\s\s+', ' ')

df.head()

def length(text):
  return len(word_tokenize(text))
df['word_count']=df['Script'].apply(length)

df.head()

def lemmatize(Words):
    new_words = []
    lem = WordNetLemmatizer()
    for w in Words:
      new_words.append(lem.lemmatize(w))
    return new_words

for w in df['Script']:
  if w in words.words():
    lemmatize(w)

df.head()

import nltk
def tokenize(text):
  return (word_tokenize(text))
df['tokenize']=df['Script'].apply(tokenize)

word2count = {}
for data in df['tokenize']:
    for i in data:
      if i not in word2count.keys():
            word2count[i] = 1
      else:
            word2count[i] += 1


print(word2count)

df

df_tf_Idf= df['Script']

# create object
tfidf = TfidfVectorizer()

# get tf-df values
result = tfidf.fit_transform(df_tf_Idf)
print(result)

# get idf values
print('\nidf values:')
for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):
    print(ele1, ':', ele2)

# get indexing
print('\nWord indexes:')
print(tfidf.vocabulary_)

# display tf-idf values
print('\ntf-idf value:')
print(result)

# in matrix form
print('\ntf-idf values in matrix form:')
print(result.toarray())

df

X = df['Script']
y = df['Label']

# split the dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=0)

print("1", X_train)
print("2", X_test)
print("3", y_train)
print("4", y_test)

tfidf = TfidfVectorizer(max_features = 270)  # Using the TF - IDF Vectorizer to extract top 5000 most important features

# Feature Extraction
X_train_tfidf = tfidf.fit_transform(X_train)  # Creating the vocabulary only from the training set to avoid data leakage from
X_test_tfidf = tfidf.transform(X_test)        # the test set.

X_train_tfidf  # Sparse Matrix is created to save memory since many values are close to 0

X_test_tfidf  # Sparse Matrix

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
tfidf_array_train = X_train_tfidf.toarray()   # Converting the sparse matrix to a numpy array (dense matrix)
tfidf_array_test = X_test_tfidf.toarray()     # Converting the sparse matrix to a numpy array (dense matrix)
scaled_X_train = scaler.fit_transform(tfidf_array_train)  # Fitting on only training data to avoid data leakage from test data
scaled_X_test = scaler.transform(tfidf_array_test) # and then tranforming both training and testing data

from sklearn.decomposition import PCA
NUM_COMPONENTS = 270   # Total number of features
pca = PCA(NUM_COMPONENTS)
reduced = pca.fit(scaled_X_train)

variance_explained = np.cumsum(pca.explained_variance_ratio_)  # Calculating the cumulative explained variance by the components

# Plotting
fig, ax = plt.subplots(figsize=(8, 6))
plt.plot(range(NUM_COMPONENTS),variance_explained, color='r')
ax.grid(True)
plt.xlabel("Number of components")
plt.ylabel("Cumulative explained variance")

final_pca = PCA(0.9)
reduced_90 = final_pca.fit_transform(scaled_X_train) # Number of Components explaining 90% variance in the training data

reduced_90_test = final_pca.transform(scaled_X_test)

reduced_90.shape

final_pca = PCA(0.8)
reduced_80 = final_pca.fit_transform(scaled_X_train) # Number of Components explaining 80% variance in the training data

reduced_80.shape

# RANDOM FORESTS
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(random_state = 42)
n_estimators = [64, 100, 128]
bootstrap = [True, False] # Bootstrapping is true by default
param_grid = {'n_estimators': n_estimators, 'bootstrap': bootstrap}
grid_rf_model = HalvingGridSearchCV(rf_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_rf_model.fit(X_train_tfidf, y_train)
preds_grid_rf_model = grid_rf_model.predict(X_test_tfidf)
print(classification_report(y_test, preds_grid_rf_model))
plot_confusion_matrix(grid_rf_model, X_test_tfidf, y_test)

# LOGISTIC REGRESSION with the the 90% variance data
from sklearn.linear_model import LogisticRegression
log_model_pca = LogisticRegression()
log_model_pca.fit(reduced_90, y_train)
preds_log_model_pca = log_model_pca.predict(reduced_90_test)
print(classification_report(y_test, preds_log_model_pca))
plot_confusion_matrix(log_model_pca, reduced_90_test, y_test)

# LOGISTIC REGRESSION with the complete data
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV
log_model = LogisticRegression(solver = 'saga')
param_grid = {'C': np.logspace(0, 10, 5)}
grid_log_model = HalvingGridSearchCV(log_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_log_model.fit(X_train_tfidf, y_train)
preds_grid_log_model = grid_log_model.predict(X_test_tfidf)
print(classification_report(y_test, preds_grid_log_model))
plot_confusion_matrix(grid_log_model, X_test_tfidf, y_test)

sgd_model = SGDClassifier()
sgd_model.fit(reduced_90,y_train)
preds_sgd_model = sgd_model.predict(reduced_90_test)
print(classification_report(y_test, preds_sgd_model))
plot_confusion_matrix(sgd_model, reduced_90_test, y_test)

cll = lgb.LGBMClassifier()
cll.fit(reduced_90, y_train)
# predict the results
y_pred=cll.predict(reduced_90_test)
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
plot_confusion_matrix(cll, reduced_90_test, y_test)

# Creating a pipeline
from sklearn.pipeline import Pipeline
pipe1 = Pipeline([('tfidf', TfidfVectorizer(max_features = 270)), ('rf_model', RandomForestClassifier(n_estimators = 128, random_state = 42))])
pipe1.fit(X, y)

x = str(input())
pipe1.predict([x])

# Creating a pipeline
from sklearn.pipeline import Pipeline
pipe2 = Pipeline([('tfidf', TfidfVectorizer(max_features = 270)), ('log_model_pca', LogisticRegression())])
pipe2.fit(X, y)

v = str(input())
pipe2.predict([v])

# Creating a pipeline
from sklearn.pipeline import Pipeline
pipe3 = Pipeline([('tfidf', TfidfVectorizer(max_features = 270)), ('sgd_model', SGDClassifier())])
pipe3.fit(X, y)

z = str(input())
pipe3.predict([z])

# Creating a pipeline
from sklearn.pipeline import Pipeline
pipe4 = Pipeline([('tfidf', TfidfVectorizer(max_features = 270)), ('cll', lgb.LGBMClassifier())])
pipe4.fit(X, y)

w = str(input())
pipe4.predict([w])

import pickle

# Serialize the model
with open("random_forest_model.pkl", "wb") as f:
    pickle.dump(grid_rf_model, f)

# Deserialize the model
with open("random_forest_model.pkl", "rb") as f:
    grid_rf_model = pickle.load(f)

from pydantic import BaseModel

class Cyberbullying(BaseModel):
    message: str
    class Config:
        schema_extra = {
            "example": {
                "message": 'i was sure this movie will be a hit but you guys made it a blockbuster.. Thankyou'
            }
        }


